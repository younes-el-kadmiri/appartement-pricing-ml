import pandas as pd
import joblib

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor

from src.preprocessing import Preprocessor
from src.modeling import ModelTrainer
from src.evaluation import Evaluator

# Ã‰tape 1 : Chargement et prÃ©paration des donnÃ©es
preprocessor = Preprocessor()
df = preprocessor.load_data('data/appartements_data_db.csv')
print("âœ… DonnÃ©es chargÃ©es")
df = preprocessor.clean_and_transform(df)
print("âœ… DonnÃ©es nettoyÃ©es et transformÃ©es")
X, y = preprocessor.select_features(df)
print(f"âœ… Features sÃ©lectionnÃ©es : {X.shape}, Target : {y.shape}")
# SÃ©paration train / test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Ã‰tape 2 : DÃ©finition des modÃ¨les
models = {
    'LinearRegression': LinearRegression(),
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'ElasticNet': ElasticNet(),
    'RandomForest': RandomForestRegressor(random_state=42),
    'ExtraTrees': ExtraTreesRegressor(random_state=42),
    'GradientBoosting': GradientBoostingRegressor(random_state=42),
    'AdaBoost': AdaBoostRegressor(random_state=42),
    'SVR': SVR(),
    'DecisionTree': DecisionTreeRegressor(random_state=42),
    'XGBoost': XGBRegressor(random_state=42)
}

# Ã‰tape 3 : EntraÃ®nement, Ã©valuation et dÃ©tection du sur-apprentissage
best_models = []

for name, model in models.items():
    print(f"\nğŸ”§ EntraÃ®nement du modÃ¨le : {name}")
    trainer = ModelTrainer(model)
    trained_model = trainer.train(X_train, y_train)

    evaluator = Evaluator(trained_model)
    train_scores = evaluator.evaluate(X_train, y_train)
    test_scores = evaluator.evaluate(X_test, y_test)

    print(f"ğŸ“Š Scores sur train :")
    Evaluator.print_scores(train_scores)
    print(f"ğŸ“‰ Scores sur test :")
    Evaluator.print_scores(test_scores)

    # VÃ©rification overfitting
    diff = train_scores['R2'] - test_scores['R2']
    if diff > 0.2:
        print(f"âš ï¸ Overfitting dÃ©tectÃ© sur {name} (diffÃ©rence R2 = {diff:.3f})")
    else:
        print(f"âœ… Pas d'overfitting dÃ©tectÃ© sur {name}")

    best_models.append((name, trained_model, test_scores['R2']))

# Ã‰tape 4 : Affichage des meilleurs modÃ¨les
best_models = sorted(best_models, key=lambda x: x[2], reverse=True)

print("\nğŸ† Top 4 modÃ¨les par R2 test :")
for i, (name, model, score) in enumerate(best_models[:4], 1):
    print(f"{i}. {name} avec R2 = {score:.4f}")

# Sauvegarde des meilleurs modÃ¨les
for i, (name, model, score) in enumerate(best_models[:4], 1):
    joblib.dump(model, f"models/model_{i}_{name}.pkl")
    print(f"ğŸ’¾ ModÃ¨le {name} sauvegardÃ© sous models/model_{i}_{name}.pkl")
